{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Shot at Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, we are going to take a baby step into the world of deep learning using PyTorch. There are a ton of notebooks out there that teach you the fundamentals of deep learning and PyTorch, so here the idea is to give you some basic introduction to deep learning and PyTorch at a very high level. Therefore, this notebook is targeting beginners but it can also serve as a review for more experienced developers.\n",
    "\n",
    "After completion of this notebook, you are expected to know the basic components of training a basic neural network with PyTorch. I have also left a couple of exercises towards the end with the intention of encouraging more research and practise of your deep learning skills. \n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Elvis Saravia([Twitter](https://twitter.com/omarsar0) | [LinkedIn](https://www.linkedin.com/in/omarsar/))\n",
    "\n",
    "**Complete Code Walkthrough:** [Blog post](https://medium.com/dair-ai/a-first-shot-at-deep-learning-with-pytorch-4a8252d30c75)\n",
    "\n",
    "## Importing the libraries\n",
    "\n",
    "Like with any other programming exercise, the first step is to import the necessary libraries. As we are going to be using Google Colab to program our neural network, we need to install and import the necessary PyTorch libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "## The usual imports\n",
    "import torch    # 导入torch\n",
    "import torch.nn as nn   # 导入torch.nn并通过nn来引用  Neural Network:神经网络\n",
    "\n",
    "## print out the pytorch version used\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=1Lpi4VPBfAV3JkOLopcsGK4L8dyxmPF1b)  \n",
    "![alt text](pytorch_hello_world_01.png)\n",
    "\n",
    "\n",
    "Before building and training a neural network the first step is to process and prepare the data. In this notebook, we are going to use syntethic data (i.e., fake data) so we won't be using any real world data. \n",
    "\n",
    "For the sake of simplicity, we are going to use the following input and output pairs converted to tensors, which is how data is typically represented in the world of deep learning. The x values represent the input of dimension `(6,1)` and the y values represent the output of similar dimension. The example is taken from this [tutorial](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb). \n",
    "\n",
    "The objective of the neural network model that we are going to build and train is to automatically learn patterns that better characterize the relationship between the `x` and `y` values. Essentially, the model learns the relationship that exists between inputs and outputs which can then be used to predict the corresponding `y` value for any given input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our data in tensor form\n",
    "x = torch.tensor([[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=torch.float)   # 定义一个2维（6*1)的tensor，数据类型为float\n",
    "y = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=torch.float)   # 定义一个2维（6*1)的tensor，数据类型为float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print size of the input tensor\n",
    "x.size()    # 打印输出张量的维度\n",
    "# print(x)\n",
    "# print('{}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network Components\n",
    "As said earlier, we are going to first define and build out the components of our neural network before training the model.\n",
    "\n",
    "### Model\n",
    "\n",
    "Typically, when building a neural network model, we define the layers and weights which form the basic components of the model. Below we show an example of how to define a hidden layer named `layer1` with size `(1, 1)`. For the purpose of this tutorial, we won't explicitly define the `weights` and allow the built-in functions provided by PyTorch to handle that part for us. By the way, the `nn.Linear(...)` function applies a linear transformation ($y = xA^T + b$) to the data that was provided as its input. We ignore the bias for now by setting `bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural network with 1 hidden layer\n",
    "layer1 = nn.Linear(1,1, bias=False) # 创建一个1维输入、一维输出的连接层--就是制造出一个全连接层的框架，即y=X*W.T + b，对给定一个具体的输入X，就会输出相应的y\n",
    "model = nn.Sequential(layer1)   # 实例化一个Sequential类\n",
    "# A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.\n",
    "# nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中，同时以神经网络模块为元素的有序字典也可以作为传入参数。\n",
    "# 因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer\n",
    "The loss function, `nn.MSELoss()`, is in charge of letting the model know how good it has learned the relationship between the input and output. The optimizer (in this case an `SGD`) primary role is to minimize or lower that loss value as it tunes its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function\n",
    "criterion = nn.MSELoss()    # 损失函数，MSE: Mean Squared Error（均方误差），是预测值与真实值之差的平方和的平均值。\n",
    "\n",
    "## optimizer algorithm\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)    # 随机梯度下降（Stochastic Gradient Descent，SGD）优化算法\n",
    "# torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "# params（必须参数）: 这是一个包含了需要优化的参数（张量）的迭代器，例如模型的参数 model.parameters()。\n",
    "# lr（必须参数）: 学习率（learning rate）。它是一个正数，控制每次参数更新的步长。较小的学习率会导致收敛较慢，较大的学习率可能导致震荡或无法收敛。\n",
    "# momentum（默认值为 0）: 动量（momentum）是一个用于加速 SGD 收敛的参数。它引入了上一步梯度的指数加权平均。通常设置在 0 到 1 之间。当 momentum 大于 0 时，算法在更新时会考虑之前的梯度，有助于加速收敛。\n",
    "# dampening（默认值为 0）: 阻尼项，用于减缓动量的速度。在某些情况下，为了防止动量项引起的震荡，可以设置一个小的 dampening 值。\n",
    "# weight_decay（默认值为 0）: 权重衰减，也称为 L2 正则化项。它用于控制参数的幅度，以防止过拟合。通常设置为一个小的正数。\n",
    "# nesterov（默认值为 False）: Nesterov 动量。当设置为 True 时，采用 Nesterov 动量更新规则。Nesterov 动量在梯度更新之前先进行一次预测，然后在计算梯度更新时使用这个预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network Model\n",
    "We have all the components we need to train our model. Below is the code used to train our model. \n",
    "\n",
    "In simple terms, we train the model by feeding it the input and output pairs for a couple of rounds (i.e., `epoch`). After a series of forward and backward steps, the model somewhat learns the relationship between x and y values. This is notable by the decrease in the computed `loss`. For a more detailed explanation of this code check out this [tutorial](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "for ITER in range(150): # 从0循环到150\n",
    "    model = model.train()   # 将模型设置为训练模式\n",
    "\n",
    "    ## forward 前向传播\n",
    "    output = model(x)   # 将输入代入模型，求出输出output\n",
    "    loss = criterion(output, y) # 求解损失函数\n",
    "    optimizer.zero_grad()   # 将梯度归零\n",
    "\n",
    "    ## backward + update model params  反向传播 + 更新模型参数\n",
    "    loss.backward()     # 反向传播计算得到每个参数的梯度值\n",
    "    optimizer.step()    # 通过梯度下降执行一步参数更新， step()函数的作用是执行一次优化步骤，通过梯度下降法来更新参数的值。因为梯度下降是基于梯度的，所以在执行optimizer.step()函数前应先执行loss.backward()函数来计算梯度。\n",
    "\n",
    "    # 调用model.eval()的作用是将模型中的某些特定层或部分切换到评估模式。在评估模式下，一些层的行为会发生变化，例如Dropout层和BatchNorm层等。这些层在训练和推断过程中的行为是不同的，因此在评估模式下需要将它们关闭。调用model.eval()会自动关闭这些层，确保在评估模型时得到正确的结果。\n",
    "    model.eval()    # 将模型设置为评估模式\n",
    "    print('Epoch: %d | Loss: %.4f' %(ITER, loss.detach().item()))   # 打印每次的损失率\n",
    "    # 使用loss.detach()来获取不需要梯度回传的部分。detach()通过重新声明一个变量，指向原变量的存放位置，但是requires_grad变为False。\n",
    "    # 使用loss.item()直接获得对应的python数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "After training the model we have the ability to test the model predictive capability by passing it an input. Below is a simple example of how you could achieve this with our model. The result we obtained aligns with the results obtained in this [notebook](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb), which inspired this entire tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.096769332885742\n"
     ]
    }
   ],
   "source": [
    "## test the model\n",
    "sample = torch.tensor([10.0], dtype=torch.float)    # 创建一个1*1的张量\n",
    "predicted = model(sample)   # 将输入sample代入模型，并求出输出predicted\n",
    "print(predicted.detach().item())    # 打印predicted的值\n",
    "# 原式： y = (2 * X) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Words\n",
    "\n",
    "Congratulations! In this tutorial you learned how to train a simple neural network using PyTorch. You also learned about the basic components that make up a neural network model such as the linear transformation layer, optimizer, and loss function. We then trained the model and tested its predictive capabilities. You are well on your way to become more knowledgeable about deep learning and PyTorch. I have provided a bunch of references below if you are interested in practising and learning more. \n",
    "\n",
    "*I would like to thank Laurence Moroney for his excellent [tutorial](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb) which I used as an inspiration for this tutorial.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "- Add more examples in the input and output tensors. In addition, try to change the dimensions of the data, say by adding an extra value in each array. What needs to be changed to successfully train the network with the new data?\n",
    "- The model converged really fast, which means it learned the relationship between x and y values after a couple of iterations. Do you think it makes sense to continue training? How would you automate the process of stopping the training after the model loss doesn't subtantially change?\n",
    "- In our example, we used a single hidden layer. Try to take a look at the PyTorch documentation to figure out what you need to do to get a model with more layers. What happens if you add more hidden layers?\n",
    "- We did not discuss the learning rate (`lr-0.001`) and the optimizer in great detail. Check out the [PyTorch documentation](https://pytorch.org/docs/stable/optim.html) to learn more about what other optimizers you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [The Hello World of Deep Learning with Neural Networks](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb)\n",
    "- [A Simple Neural Network from Scratch with PyTorch and Google Colab](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0?source=collection_category---4------1-----------------------)\n",
    "- [PyTorch Official Docs](https://pytorch.org/docs/stable/nn.html)\n",
    "- [PyTorch 1.2 Quickstart with Google Colab](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d)\n",
    "- [A Gentle Intoduction to PyTorch](https://medium.com/dair-ai/pytorch-1-2-introduction-guide-f6fa9bb7597c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ade285f687a1ecab6f569c64721a8142b161535723b6a0ecd56d473b77660bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
