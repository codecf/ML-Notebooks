{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1br0hM79ORTVNXUpVgkV5t4o4AigGxfwk?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m41136O7L5bV"
      },
      "source": [
        "In this tutorial, we are going to implement a linear regression model to predict california housing prices. We will build the model from scratch using numpy. This will be a great approach to begin understanding regression based models.\n",
        "\n",
        "After completing this tutorial the learner is expected to know the basic building blocks of a linear regression model. The learner is also expected to know the pipeline of reading and transforming data for machine learning workflows.  \n",
        "\n",
        "1. 学习使用线性回归模型预测加州的房价--掌握关于线性回归模型的基础模块  \n",
        "2. 了解--know the pipeline of reading and transforming data for machine learning workflows\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Pni17h4R8v8a"
      },
      "outputs": [],
      "source": [
        "## Import the usual libraries\n",
        "import numpy as np                  # 处理数组和矩阵\n",
        "import pandas as pd                 # Pandas 是 Python 的核心数据分析支持库，提供了快速、灵活、明确的数据结构，旨在简单、直观地处理关系型、标记型数据。Pandas 的主要数据结构是 Series（一维数据）与 DataFrame（二维数据）.\n",
        "from sklearn.datasets import fetch_california_housing   # 导入 fetch_california_housing 数据集\n",
        "from sklearn.model_selection import train_test_split    # 导入数据切分函数\n",
        "from sklearn.preprocessing import StandardScaler        # 导入归一化工具\n",
        "import matplotlib.pyplot as plt                         # 绘图"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjyJUfczL4zX"
      },
      "source": [
        "# Importing the dataset\n",
        "\n",
        "The real-world dataset can be obtained by the function `fetch_california_housing` that downloads the dataset for us. \n",
        "\n",
        "The `as_frame` parameter returns a pandas dataframe which is a library useful for viewing contents of the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aOXxbywahC5X",
        "outputId": "24521e0a-6f1a-4e5c-c35d-3abb8112a9af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedHouseVal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>4.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>3.585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>3.521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  MedHouseVal  \n",
              "0    -122.23        4.526  \n",
              "1    -122.22        3.585  \n",
              "2    -122.24        3.521  \n",
              "3    -122.25        3.413  \n",
              "4    -122.25        3.422  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fetch the data using sklearn function\n",
        "bunch = fetch_california_housing(download_if_missing=True, as_frame=True)   # 使用 sklearn 的函数下载数据\n",
        "\n",
        "# Load the dataframe and view\n",
        "df = bunch.frame    # 加载数据集，转换为dataframe格式(pandas 库)\n",
        "# 数据名.head( ) ：是指取数据的前n行数据，默认是前5行。需要注意的是没有print语句，python中的head（）函数只是选择数据，而不对数据内容做任何改变。\n",
        "df.head()           # 打印前五行数据\n",
        "# 人均收入(MedInc)、房龄(HouseAge)、房间数(AveRooms)、卧室数(AveBedrooms)、小区人口数(Population)、房屋居住人数(AveOccup)、小区经度(Longitude)、小区纬度(Latitude)和房价中位数(Median_house_value)。其中，房价中位数是标签，其余的8个变量均为特征。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUeU_jLylTx7"
      },
      "source": [
        "For this dataset, our target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
        "\n",
        "We can take a closer look at the various statistical parameters of the dataset using pandas. The `describe` function will help us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "eD4BpBHClgDc",
        "outputId": "e2171ffb-bd38-4e3e-b600-ad3c249a3234"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedHouseVal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.870671</td>\n",
              "      <td>28.639486</td>\n",
              "      <td>5.429000</td>\n",
              "      <td>1.096675</td>\n",
              "      <td>1425.476744</td>\n",
              "      <td>3.070655</td>\n",
              "      <td>35.631861</td>\n",
              "      <td>-119.569704</td>\n",
              "      <td>2.068558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.899822</td>\n",
              "      <td>12.585558</td>\n",
              "      <td>2.474173</td>\n",
              "      <td>0.473911</td>\n",
              "      <td>1132.462122</td>\n",
              "      <td>10.386050</td>\n",
              "      <td>2.135952</td>\n",
              "      <td>2.003532</td>\n",
              "      <td>1.153956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.499900</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>32.540000</td>\n",
              "      <td>-124.350000</td>\n",
              "      <td>0.149990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.563400</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>4.440716</td>\n",
              "      <td>1.006079</td>\n",
              "      <td>787.000000</td>\n",
              "      <td>2.429741</td>\n",
              "      <td>33.930000</td>\n",
              "      <td>-121.800000</td>\n",
              "      <td>1.196000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.534800</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>5.229129</td>\n",
              "      <td>1.048780</td>\n",
              "      <td>1166.000000</td>\n",
              "      <td>2.818116</td>\n",
              "      <td>34.260000</td>\n",
              "      <td>-118.490000</td>\n",
              "      <td>1.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.743250</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>6.052381</td>\n",
              "      <td>1.099526</td>\n",
              "      <td>1725.000000</td>\n",
              "      <td>3.282261</td>\n",
              "      <td>37.710000</td>\n",
              "      <td>-118.010000</td>\n",
              "      <td>2.647250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>15.000100</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>141.909091</td>\n",
              "      <td>34.066667</td>\n",
              "      <td>35682.000000</td>\n",
              "      <td>1243.333333</td>\n",
              "      <td>41.950000</td>\n",
              "      <td>-114.310000</td>\n",
              "      <td>5.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
              "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
              "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
              "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
              "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
              "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
              "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
              "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
              "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
              "\n",
              "           AveOccup      Latitude     Longitude   MedHouseVal  \n",
              "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
              "mean       3.070655     35.631861   -119.569704      2.068558  \n",
              "std       10.386050      2.135952      2.003532      1.153956  \n",
              "min        0.692308     32.540000   -124.350000      0.149990  \n",
              "25%        2.429741     33.930000   -121.800000      1.196000  \n",
              "50%        2.818116     34.260000   -118.490000      1.797000  \n",
              "75%        3.282261     37.710000   -118.010000      2.647250  \n",
              "max     1243.333333     41.950000   -114.310000      5.000010  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()   # 返回数据集的统计变量\n",
        "# count：数量统计，此列共有多少有效值\n",
        "# mean：均值\n",
        "# std：标准差\n",
        "# min：最小值\n",
        "# 25%：四分之一分位数\n",
        "# 50%：二分之一分位数\n",
        "# 75%：四分之三分位数\n",
        "# max：最大值\n",
        "# unipue：不同的值有多少个"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6WG0Bejxc2"
      },
      "source": [
        "As we can see the data in each of the columns is on different scales. For example, the average bedroom value is around 1 and the average population is around 1425. \n",
        "\n",
        "Generally, machine learing models do not work well when the data is on different scales. Thus, we have to normalize our data in the range [-1,1]. The module [StandardScalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) will help us in this.  \n",
        "normalize data : 归一化 , 使用 [StandardScalar] 模块\n",
        "\n",
        "The training data should always be normalized. The testing data should be normalized using the values of the training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pkaOgN44iQLN"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/Ankit152/Fish-Market/main/Fish.csv\n",
        "# import pandas as pd\n",
        "# df  = pd.read_csv(\"Fish.csv\")\n",
        "# y = df['Weight']\n",
        "# x = df[[\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\",\"Weight\"]]\n",
        "\n",
        "df = bunch.frame    # 加载数据集，转换为dataframe格式(pandas 库)\n",
        "x = df.iloc[:,:-1]  # Select all the columns, except the last column    选择除最后一列外的所有列作为特征向量\n",
        "y = df.iloc[:,-1:]  # Select the last column                            选择最后一列作为标记向量\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 1)   # 按照测试集占0.33的比例划分训练集和测试集\n",
        "\n",
        "input_scalar = StandardScaler()     # 实例化 StandardScaler 类，用于处理输入（归一化、反归一化）\n",
        "output_scalar = StandardScaler()    # 实例化 StandardScaler 类，用于处理输出（归一化、反归一化）\n",
        "\n",
        "x_train = input_scalar.fit_transform(x_train).T # Normalize train data                          归一化训练数据的输入向量(特征向量); .fit_transform():进行标准化，降维、归一化等操作; .T将矩阵转置\n",
        "x_test = input_scalar.transform(x_test).T # Only transform test data using values of train data 归一化测试数据的输入向量(特征向量)\n",
        "\n",
        "y_train = output_scalar.fit_transform(y_train).reshape(-1)  # 归一化训练数据的输出向量(标记); .fit_transform():进行标准化，降维、归一化等操作; reshape(-1) : 更改数组的形状,变为1列\n",
        "y_test = output_scalar.transform(y_test).reshape(-1)        # 归一化测试数据的输出向量(标记); \n",
        "\n",
        "dataset_copy = [ x_train.copy(), x_test.copy(),  y_train.copy(),  y_test.copy()]    # 将归一化的数据copy到dataset_copy数组中 用于后续验证sklearn.linear_model 线性回归库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mylVXZDk96a2"
      },
      "source": [
        "# Linear Regression Model\n",
        "\n",
        "Now we define our linear regression model from scratch.\n",
        "\n",
        "A linear regression model is of the form:\n",
        "\n",
        "$y = a_1 x_1 + a_2 x_2 + \\dots + a_nx_n + a_{n+1}$\n",
        "  \n",
        "The above can be rewritten using matrix multiplication as\n",
        "\n",
        "$ y = w^T x $\n",
        "\n",
        "where \n",
        "\n",
        "$ w = [a_1, a_2, \\dots,  a_n, a_{n+1}]^T $\n",
        "\n",
        "$ x = [x_1, x_2, \\dots,  x_n]^T $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "iJViSowz9nah"
      },
      "outputs": [],
      "source": [
        "class LinearRegression():   # 定义线性回归类\n",
        "  def __init__(self, dim, lr = 0.1):  # 初始化  dim:维度； lr：学习率\n",
        "    assert isinstance # 断言 assert ； isinstance()函数是Python中的一个内置函数，用于判断一个对象是否是指定类型的实例\n",
        "    self.lr = lr      # 初始化学习率\n",
        "    self.w = np.zeros((dim))  # 创建一个dim维，初始值为0的参数向量w\n",
        "    self.grads = {\"dw\": np.zeros((dim)) +5} # 创建grads(梯度)字典数据结构--保存损失值 loss； key值:\"dw\"; 值为 w 向量+5 （向量加法，等同于同维值为5的向量)\n",
        "\n",
        "\n",
        "  def forward(self, x): # 前向传播，即计算预测值y\n",
        "    y = self.w.T @ x    # @ 标识矩阵乘法; 即 y = wT * x\n",
        "    return y\n",
        "  \n",
        "  def backward(self, x, y_hat, y):  # 梯度下降法进行反向传播 x:特征向量; y_hat:预测值; y:标记 \n",
        "    assert y_hat.shape == y.shape   # 如果 y_hat 的形状和 y 的形状不相等，进行assert\n",
        "    self.grads[\"dw\"] = (1 / x.shape[1]) * ((y_hat - y) @ x.T).T   # 梯度下降法 推导可参见 https://blog.csdn.net/qq_42359956/article/details/105971920\n",
        "    assert self.grads[\"dw\"].shape == self.w.shape   # 如果grads[\"dw\"] 的形状和 w 的形状不相等，进行assert\n",
        "    \n",
        "    # print(self.grads[\"dw\"])\n",
        "\n",
        "  def optimize(self): # 优化参数\n",
        "    self.w = self.w - self.lr * self.grads[\"dw\"]  # 使用梯度更新参数w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UOy32LoqCrL"
      },
      "source": [
        "# Loss\n",
        "\n",
        "For linear regression, various loss functions such as the mean absolute error, mean squared error, or root mean squared error can be used.\n",
        "\n",
        "In this example, we will use the mean squared error (MSE) loss.\n",
        "\n",
        "The MSE loss is given by \n",
        "\n",
        "$ error = \\frac{1}{m} Σ_{i=1}^{m} (y_{true}^{i} - y_{pred}^{i})^2 $ \n",
        "\n",
        "where $i$ denotes the particular obseration/row in the dataset and $m$ is the total number of obserations.\n",
        "\n",
        "To ensure our model is correct, the loss should decrease over each epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3atqq0KirwLu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mgNn3oGjjbxX",
        "outputId": "2f67998e-2fa4-4c43-e091-d5180dd92029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Train Loss 0.49999999999999983 | Test Loss 0.43712002508129305\n",
            "Epoch 20 | Train Loss 0.2359010824314295 | Test Loss 0.2325611904818386\n",
            "Epoch 40 | Train Loss 0.22188755162559423 | Test Loss 0.2217635127918686\n",
            "Epoch 60 | Train Loss 0.21474640752415047 | Test Loss 0.2152613227580797\n",
            "Epoch 80 | Train Loss 0.2095989567021037 | Test Loss 0.21037193610067245\n",
            "Epoch 100 | Train Loss 0.20581761895152345 | Test Loss 0.20673038702760732\n",
            "Epoch 120 | Train Loss 0.20303294882659725 | Test Loss 0.20402527473733864\n",
            "Epoch 140 | Train Loss 0.20097918345162274 | Test Loss 0.20201418597478873\n",
            "Epoch 160 | Train Loss 0.19946210112703647 | Test Loss 0.20051638978054404\n",
            "Epoch 180 | Train Loss 0.19833950070910428 | Test Loss 0.19939846923590557\n",
            "Epoch 200 | Train Loss 0.19750719045004836 | Test Loss 0.19856209982480624\n",
            "Epoch 220 | Train Loss 0.1968887724916989 | Test Loss 0.1979347869242644\n",
            "Epoch 240 | Train Loss 0.19642818272324772 | Test Loss 0.19746302071881933\n",
            "Epoch 260 | Train Loss 0.19608424065206406 | Test Loss 0.19710724843033994\n",
            "Epoch 280 | Train Loss 0.19582666642954677 | Test Loss 0.19683818623685323\n",
            "Epoch 300 | Train Loss 0.19563316903289557 | Test Loss 0.19663411372213185\n",
            "Epoch 320 | Train Loss 0.19548731666143612 | Test Loss 0.19647888801910168\n",
            "Epoch 340 | Train Loss 0.19537697849050284 | Test Loss 0.19636048509166582\n",
            "Epoch 360 | Train Loss 0.19529318388491884 | Test Loss 0.19626992725672532\n",
            "Epoch 380 | Train Loss 0.19522928672715056 | Test Loss 0.19620049386222657\n",
            "Epoch 400 | Train Loss 0.19518035283239818 | Test Loss 0.19614713968570663\n",
            "Epoch 420 | Train Loss 0.19514271054504922 | Test Loss 0.1961060658296224\n",
            "Epoch 440 | Train Loss 0.19511362075549118 | Test Loss 0.19607440266698017\n",
            "Epoch 460 | Train Loss 0.19509103436032973 | Test Loss 0.196049975197568\n",
            "Epoch 480 | Train Loss 0.19507341379189 | Test Loss 0.1960311290809374\n",
            "Epoch 500 | Train Loss 0.195059601524607 | Test Loss 0.1960166013981983\n",
            "Epoch 520 | Train Loss 0.19504872305387622 | Test Loss 0.19600542443105118\n",
            "Epoch 540 | Train Loss 0.19504011519474806 | Test Loss 0.19599685384985982\n",
            "Epoch 560 | Train Loss 0.19503327299733605 | Test Loss 0.19599031497728384\n",
            "Epoch 580 | Train Loss 0.19502781036651573 | Test Loss 0.19598536246238987\n",
            "Epoch 600 | Train Loss 0.19502343078313097 | Test Loss 0.19598164992474956\n",
            "Epoch 620 | Train Loss 0.19501990548217418 | Test Loss 0.19597890702760307\n",
            "Epoch 640 | Train Loss 0.19501705714492468 | Test Loss 0.1959769221005947\n",
            "Epoch 660 | Train Loss 0.1950147476759042 | Test Loss 0.1959755289194148\n",
            "Epoch 680 | Train Loss 0.19501286901218934 | Test Loss 0.19597459660841743\n",
            "Epoch 700 | Train Loss 0.1950113361889526 | Test Loss 0.19597402189697258\n",
            "Epoch 720 | Train Loss 0.1950100820879975 | Test Loss 0.19597372315589145\n",
            "Epoch 740 | Train Loss 0.1950090534451732 | Test Loss 0.19597363578501353\n",
            "Epoch 760 | Train Loss 0.1950082078022795 | Test Loss 0.19597370863036628\n",
            "Epoch 780 | Train Loss 0.19500751116991094 | Test Loss 0.19597390118903935\n",
            "Epoch 800 | Train Loss 0.19500693622732843 | Test Loss 0.19597418141927686\n",
            "Epoch 820 | Train Loss 0.19500646092952323 | Test Loss 0.19597452401759977\n",
            "Epoch 840 | Train Loss 0.19500606742426427 | Test Loss 0.1959749090579219\n",
            "Epoch 860 | Train Loss 0.19500574120612196 | Test Loss 0.19597532091250944\n",
            "Epoch 880 | Train Loss 0.19500547045245514 | Test Loss 0.19597574739336301\n",
            "Epoch 900 | Train Loss 0.19500524549975556 | Test Loss 0.1959761790667557\n",
            "Epoch 920 | Train Loss 0.19500505842876395 | Test Loss 0.19597660870438502\n",
            "Epoch 940 | Train Loss 0.1950049027342804 | Test Loss 0.1959770308427642\n",
            "Epoch 960 | Train Loss 0.19500477306123667 | Test Loss 0.1959774414287147\n",
            "Epoch 980 | Train Loss 0.19500466499285687 | Test Loss 0.19597783753361098\n",
            "Epoch 1000 | Train Loss 0.19500457487995757 | Test Loss 0.19597821712271776\n",
            "Epoch 1020 | Train Loss 0.1950044997028894 | Test Loss 0.19597857886881814\n",
            "Epoch 1040 | Train Loss 0.1950044369594934 | Test Loss 0.19597892200155245\n",
            "Epoch 1060 | Train Loss 0.19500438457387914 | Test Loss 0.19597924618562568\n",
            "Epoch 1080 | Train Loss 0.1950043408219371 | Test Loss 0.1959795514224043\n",
            "Epoch 1100 | Train Loss 0.19500430427035154 | Test Loss 0.19597983797049975\n",
            "Epoch 1120 | Train Loss 0.19500427372654458 | Test Loss 0.1959801062817876\n",
            "Epoch 1140 | Train Loss 0.195004248197501 | Test Loss 0.19598035694999025\n",
            "Epoch 1160 | Train Loss 0.19500422685583052 | Test Loss 0.1959805906694935\n",
            "Epoch 1180 | Train Loss 0.1950042090117447 | Test Loss 0.19598080820250163\n",
            "Epoch 1200 | Train Loss 0.195004194089881 | Test Loss 0.1959810103529865\n",
            "Epoch 1220 | Train Loss 0.19500418161010774 | Test Loss 0.1959811979461702\n",
            "Epoch 1240 | Train Loss 0.1950041711716055 | Test Loss 0.19598137181250938\n",
            "Epoch 1260 | Train Loss 0.1950041624396518 | Test Loss 0.1959815327753366\n",
            "Epoch 1280 | Train Loss 0.19500415513463745 | Test Loss 0.19598168164146745\n",
            "Epoch 1300 | Train Loss 0.19500414902293145 | Test Loss 0.19598181919420496\n",
            "Epoch 1320 | Train Loss 0.1950041439092753 | Test Loss 0.19598194618827683\n",
            "Epoch 1340 | Train Loss 0.19500413963044858 | Test Loss 0.19598206334632265\n",
            "Epoch 1360 | Train Loss 0.1950041360499879 | Test Loss 0.19598217135661974\n",
            "Epoch 1380 | Train Loss 0.19500413305378356 | Test Loss 0.19598227087179027\n",
            "Epoch 1400 | Train Loss 0.1950041305464049 | Test Loss 0.19598236250828016\n",
            "Epoch 1420 | Train Loss 0.1950041284480337 | Test Loss 0.19598244684643956\n",
            "Epoch 1440 | Train Loss 0.19500412669190298 | Test Loss 0.19598252443106423\n",
            "Epoch 1460 | Train Loss 0.1950041252221582 | Test Loss 0.1959825957722857\n",
            "Epoch 1480 | Train Loss 0.1950041239920701 | Test Loss 0.19598266134671743\n",
            "Epoch 1500 | Train Loss 0.1950041229625413 | Test Loss 0.19598272159878383\n",
            "Epoch 1520 | Train Loss 0.19500412210085774 | Test Loss 0.19598277694217195\n",
            "Epoch 1540 | Train Loss 0.19500412137964543 | Test Loss 0.19598282776135909\n",
            "Epoch 1560 | Train Loss 0.19500412077599738 | Test Loss 0.19598287441317852\n",
            "Epoch 1580 | Train Loss 0.19500412027074418 | Test Loss 0.19598291722839395\n",
            "Epoch 1600 | Train Loss 0.19500411984784347 | Test Loss 0.1959829565132598\n",
            "Epoch 1620 | Train Loss 0.19500411949386964 | Test Loss 0.19598299255105023\n",
            "Epoch 1640 | Train Loss 0.19500411919758648 | Test Loss 0.1959830256035433\n",
            "Epoch 1660 | Train Loss 0.19500411894959 | Test Loss 0.1959830559124517\n",
            "Epoch 1680 | Train Loss 0.19500411874200962 | Test Loss 0.19598308370079265\n",
            "Epoch 1700 | Train Loss 0.19500411856825786 | Test Loss 0.19598310917419287\n",
            "Epoch 1720 | Train Loss 0.19500411842282128 | Test Loss 0.1959831325221266\n",
            "Epoch 1740 | Train Loss 0.19500411830108505 | Test Loss 0.19598315391908486\n",
            "Epoch 1760 | Train Loss 0.19500411819918667 | Test Loss 0.19598317352567718\n",
            "Epoch 1780 | Train Loss 0.19500411811389318 | Test Loss 0.19598319148966561\n",
            "Epoch 1800 | Train Loss 0.19500411804249856 | Test Loss 0.19598320794693336\n",
            "Epoch 1820 | Train Loss 0.19500411798273784 | Test Loss 0.1959832230223899\n",
            "Epoch 1840 | Train Loss 0.19500411793271513 | Test Loss 0.19598323683081562\n",
            "Epoch 1860 | Train Loss 0.19500411789084354 | Test Loss 0.19598324947764745\n",
            "Epoch 1880 | Train Loss 0.19500411785579488 | Test Loss 0.19598326105971003\n",
            "Epoch 1900 | Train Loss 0.19500411782645727 | Test Loss 0.19598327166589385\n",
            "Epoch 1920 | Train Loss 0.19500411780190013 | Test Loss 0.1959832813777842\n",
            "Epoch 1940 | Train Loss 0.1950041177813444 | Test Loss 0.1959832902702435\n",
            "Epoch 1960 | Train Loss 0.1950041177641382 | Test Loss 0.1959832984119505\n",
            "Epoch 1980 | Train Loss 0.19500411774973558 | Test Loss 0.19598330586589785\n",
            "Epoch 2000 | Train Loss 0.1950041177376798 | Test Loss 0.1959833126898522\n",
            "Epoch 2020 | Train Loss 0.19500411772758847 | Test Loss 0.1959833189367787\n",
            "Epoch 2040 | Train Loss 0.19500411771914136 | Test Loss 0.19598332465523224\n",
            "Epoch 2060 | Train Loss 0.1950041177120707 | Test Loss 0.19598332988971823\n",
            "Epoch 2080 | Train Loss 0.1950041177061521 | Test Loss 0.1959833346810246\n",
            "Epoch 2100 | Train Loss 0.195004117701198 | Test Loss 0.19598333906652787\n",
            "Epoch 2120 | Train Loss 0.19500411769705106 | Test Loss 0.19598334308047438\n",
            "Epoch 2140 | Train Loss 0.1950041176935798 | Test Loss 0.19598334675423898\n",
            "Epoch 2160 | Train Loss 0.19500411769067422 | Test Loss 0.1959833501165632\n",
            "Epoch 2180 | Train Loss 0.19500411768824202 | Test Loss 0.1959833531937735\n",
            "Epoch 2200 | Train Loss 0.19500411768620615 | Test Loss 0.19598335600998243\n",
            "Epoch 2220 | Train Loss 0.19500411768450196 | Test Loss 0.1959833585872731\n",
            "Epoch 2240 | Train Loss 0.19500411768307552 | Test Loss 0.19598336094586813\n",
            "Epoch 2260 | Train Loss 0.19500411768188147 | Test Loss 0.19598336310428585\n",
            "Epoch 2280 | Train Loss 0.19500411768088202 | Test Loss 0.195983365079482\n",
            "Epoch 2300 | Train Loss 0.19500411768004536 | Test Loss 0.19598336688698145\n",
            "Epoch 2320 | Train Loss 0.19500411767934506 | Test Loss 0.1959833685409978\n",
            "Epoch 2340 | Train Loss 0.19500411767875886 | Test Loss 0.19598337005454344\n",
            "Epoch 2360 | Train Loss 0.1950041176782682 | Test Loss 0.1959833714395307\n",
            "Epoch 2380 | Train Loss 0.1950041176778575 | Test Loss 0.1959833727068645\n",
            "Epoch 2400 | Train Loss 0.19500411767751366 | Test Loss 0.19598337386652676\n",
            "Epoch 2420 | Train Loss 0.19500411767722584 | Test Loss 0.1959833749276544\n",
            "Epoch 2440 | Train Loss 0.19500411767698494 | Test Loss 0.1959833758986108\n",
            "Epoch 2460 | Train Loss 0.19500411767678333 | Test Loss 0.19598337678705066\n",
            "Epoch 2480 | Train Loss 0.19500411767661452 | Test Loss 0.19598337759998\n",
            "Epoch 2500 | Train Loss 0.19500411767647324 | Test Loss 0.19598337834381113\n",
            "Epoch 2520 | Train Loss 0.195004117676355 | Test Loss 0.19598337902441254\n",
            "Epoch 2540 | Train Loss 0.195004117676256 | Test Loss 0.1959833796471551\n",
            "Epoch 2560 | Train Loss 0.19500411767617312 | Test Loss 0.19598338021695402\n",
            "Epoch 2580 | Train Loss 0.1950041176761038 | Test Loss 0.19598338073830746\n",
            "Epoch 2600 | Train Loss 0.19500411767604572 | Test Loss 0.19598338121533196\n",
            "Epoch 2620 | Train Loss 0.19500411767599712 | Test Loss 0.19598338165179446\n",
            "Epoch 2640 | Train Loss 0.1950041176759564 | Test Loss 0.19598338205114224\n",
            "Epoch 2660 | Train Loss 0.19500411767592238 | Test Loss 0.19598338241652982\n",
            "Epoch 2680 | Train Loss 0.19500411767589387 | Test Loss 0.19598338275084387\n",
            "Epoch 2700 | Train Loss 0.19500411767587003 | Test Loss 0.1959833830567258\n",
            "Epoch 2720 | Train Loss 0.19500411767585005 | Test Loss 0.19598338333659285\n",
            "Epoch 2740 | Train Loss 0.1950041176758333 | Test Loss 0.19598338359265657\n",
            "Epoch 2760 | Train Loss 0.19500411767581935 | Test Loss 0.19598338382694094\n",
            "Epoch 2780 | Train Loss 0.19500411767580764 | Test Loss 0.19598338404129775\n",
            "Epoch 2800 | Train Loss 0.19500411767579784 | Test Loss 0.19598338423742154\n",
            "Epoch 2820 | Train Loss 0.19500411767578962 | Test Loss 0.19598338441686272\n",
            "Epoch 2840 | Train Loss 0.19500411767578277 | Test Loss 0.19598338458104\n",
            "Epoch 2860 | Train Loss 0.195004117675777 | Test Loss 0.1959833847312515\n",
            "Epoch 2880 | Train Loss 0.1950041176757722 | Test Loss 0.19598338486868488\n",
            "Epoch 2900 | Train Loss 0.19500411767576814 | Test Loss 0.19598338499442705\n",
            "Epoch 2920 | Train Loss 0.19500411767576475 | Test Loss 0.19598338510947227\n",
            "Epoch 2940 | Train Loss 0.19500411767576192 | Test Loss 0.19598338521473044\n",
            "Epoch 2960 | Train Loss 0.1950041176757596 | Test Loss 0.19598338531103396\n",
            "Epoch 2980 | Train Loss 0.19500411767575762 | Test Loss 0.19598338539914462\n",
            "Epoch 3000 | Train Loss 0.19500411767575593 | Test Loss 0.1959833854797592\n",
            "Epoch 3020 | Train Loss 0.19500411767575457 | Test Loss 0.1959833855535154\n",
            "Epoch 3040 | Train Loss 0.1950041176757534 | Test Loss 0.1959833856209966\n",
            "Epoch 3060 | Train Loss 0.19500411767575246 | Test Loss 0.19598338568273665\n",
            "Epoch 3080 | Train Loss 0.19500411767575163 | Test Loss 0.19598338573922397\n",
            "Epoch 3100 | Train Loss 0.19500411767575093 | Test Loss 0.19598338579090527\n",
            "Epoch 3120 | Train Loss 0.19500411767575035 | Test Loss 0.19598338583818956\n",
            "Epoch 3140 | Train Loss 0.19500411767574988 | Test Loss 0.19598338588145087\n",
            "Epoch 3160 | Train Loss 0.1950041176757495 | Test Loss 0.1959833859210314\n",
            "Epoch 3180 | Train Loss 0.19500411767574916 | Test Loss 0.19598338595724435\n",
            "Epoch 3200 | Train Loss 0.1950041176757489 | Test Loss 0.19598338599037615\n",
            "Epoch 3220 | Train Loss 0.19500411767574863 | Test Loss 0.19598338602068902\n",
            "Epoch 3240 | Train Loss 0.19500411767574843 | Test Loss 0.19598338604842275\n",
            "Epoch 3260 | Train Loss 0.19500411767574832 | Test Loss 0.19598338607379676\n",
            "Epoch 3280 | Train Loss 0.19500411767574816 | Test Loss 0.19598338609701185\n",
            "Epoch 3300 | Train Loss 0.19500411767574805 | Test Loss 0.19598338611825167\n",
            "Epoch 3320 | Train Loss 0.19500411767574796 | Test Loss 0.19598338613768423\n",
            "Epoch 3340 | Train Loss 0.19500411767574785 | Test Loss 0.19598338615546346\n",
            "Epoch 3360 | Train Loss 0.1950041176757478 | Test Loss 0.1959833861717299\n",
            "Epoch 3380 | Train Loss 0.1950041176757477 | Test Loss 0.19598338618661224\n",
            "Epoch 3400 | Train Loss 0.19500411767574768 | Test Loss 0.1959833862002283\n",
            "Epoch 3420 | Train Loss 0.19500411767574766 | Test Loss 0.19598338621268585\n",
            "Epoch 3440 | Train Loss 0.19500411767574763 | Test Loss 0.19598338622408337\n",
            "Epoch 3460 | Train Loss 0.1950041176757476 | Test Loss 0.1959833862345112\n",
            "Epoch 3480 | Train Loss 0.1950041176757476 | Test Loss 0.19598338624405162\n",
            "Epoch 3500 | Train Loss 0.19500411767574752 | Test Loss 0.19598338625278036\n",
            "Epoch 3520 | Train Loss 0.19500411767574755 | Test Loss 0.19598338626076636\n",
            "Epoch 3540 | Train Loss 0.19500411767574752 | Test Loss 0.19598338626807282\n",
            "Epoch 3560 | Train Loss 0.19500411767574752 | Test Loss 0.19598338627475767\n",
            "Epoch 3580 | Train Loss 0.1950041176757475 | Test Loss 0.19598338628087364\n",
            "Epoch 3600 | Train Loss 0.1950041176757475 | Test Loss 0.19598338628646922\n",
            "Epoch 3620 | Train Loss 0.19500411767574746 | Test Loss 0.1959833862915887\n",
            "Epoch 3640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338629627257\n",
            "Epoch 3660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338630055787\n",
            "Epoch 3680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338630447856\n",
            "Epoch 3700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338630806564\n",
            "Epoch 3720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338631134748\n",
            "Epoch 3740 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863143501\n",
            "Epoch 3760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338631709722\n",
            "Epoch 3780 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863196106\n",
            "Epoch 3800 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863219101\n",
            "Epoch 3820 | Train Loss 0.19500411767574746 | Test Loss 0.19598338632401394\n",
            "Epoch 3840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338632593876\n",
            "Epoch 3860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338632769982\n",
            "Epoch 3880 | Train Loss 0.19500411767574743 | Test Loss 0.19598338632931098\n",
            "Epoch 3900 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863307851\n",
            "Epoch 3920 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633213377\n",
            "Epoch 3940 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633336768\n",
            "Epoch 3960 | Train Loss 0.19500411767574743 | Test Loss 0.19598338633449658\n",
            "Epoch 3980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633552945\n",
            "Epoch 4000 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633647439\n",
            "Epoch 4020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633733897\n",
            "Epoch 4040 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633812995\n",
            "Epoch 4060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633885368\n",
            "Epoch 4080 | Train Loss 0.19500411767574746 | Test Loss 0.19598338633951579\n",
            "Epoch 4100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634012158\n",
            "Epoch 4120 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863406758\n",
            "Epoch 4140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634118284\n",
            "Epoch 4160 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863416468\n",
            "Epoch 4180 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634207124\n",
            "Epoch 4200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634245957\n",
            "Epoch 4220 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863428149\n",
            "Epoch 4240 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634313991\n",
            "Epoch 4260 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634343732\n",
            "Epoch 4280 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863437094\n",
            "Epoch 4300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634395834\n",
            "Epoch 4320 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863441861\n",
            "Epoch 4340 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863443945\n",
            "Epoch 4360 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634458515\n",
            "Epoch 4380 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634475954\n",
            "Epoch 4400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634491913\n",
            "Epoch 4420 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634506512\n",
            "Epoch 4440 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863451987\n",
            "Epoch 4460 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634532095\n",
            "Epoch 4480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634543272\n",
            "Epoch 4500 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634553503\n",
            "Epoch 4520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634562865\n",
            "Epoch 4540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634571427\n",
            "Epoch 4560 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863457926\n",
            "Epoch 4580 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634586435\n",
            "Epoch 4600 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863459299\n",
            "Epoch 4620 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634598989\n",
            "Epoch 4640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634604479\n",
            "Epoch 4660 | Train Loss 0.19500411767574746 | Test Loss 0.195983386346095\n",
            "Epoch 4680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634614093\n",
            "Epoch 4700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634618298\n",
            "Epoch 4720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634622145\n",
            "Epoch 4740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634625664\n",
            "Epoch 4760 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863462888\n",
            "Epoch 4780 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863463183\n",
            "Epoch 4800 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634634527\n",
            "Epoch 4820 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863463699\n",
            "Epoch 4840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634639245\n",
            "Epoch 4860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634641307\n",
            "Epoch 4880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634643198\n",
            "Epoch 4900 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634644927\n",
            "Epoch 4920 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634646506\n",
            "Epoch 4940 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863464795\n",
            "Epoch 4960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634649273\n",
            "Epoch 4980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634650483\n",
            "Epoch 5000 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634651596\n",
            "Epoch 5020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634652604\n",
            "Epoch 5040 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863465353\n",
            "Epoch 5060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634654378\n",
            "Epoch 5080 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863465516\n",
            "Epoch 5100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634655868\n",
            "Epoch 5120 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634656515\n",
            "Epoch 5140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634657111\n",
            "Epoch 5160 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634657653\n",
            "Epoch 5180 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863465815\n",
            "Epoch 5200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634658605\n",
            "Epoch 5220 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634659024\n",
            "Epoch 5240 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634659407\n",
            "Epoch 5260 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634659754\n",
            "Epoch 5280 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634660073\n",
            "Epoch 5300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634660364\n",
            "Epoch 5320 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466063\n",
            "Epoch 5340 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634660875\n",
            "Epoch 5360 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634661094\n",
            "Epoch 5380 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634661303\n",
            "Epoch 5400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634661489\n",
            "Epoch 5420 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634661658\n",
            "Epoch 5440 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634661816\n",
            "Epoch 5460 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466196\n",
            "Epoch 5480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662088\n",
            "Epoch 5500 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466221\n",
            "Epoch 5520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662318\n",
            "Epoch 5540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662418\n",
            "Epoch 5560 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466251\n",
            "Epoch 5580 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662596\n",
            "Epoch 5600 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662674\n",
            "Epoch 5620 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634662743\n",
            "Epoch 5640 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466281\n",
            "Epoch 5660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634662868\n",
            "Epoch 5680 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466292\n",
            "Epoch 5700 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466297\n",
            "Epoch 5720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663018\n",
            "Epoch 5740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663057\n",
            "Epoch 5760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663093\n",
            "Epoch 5780 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663132\n",
            "Epoch 5800 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466316\n",
            "Epoch 5820 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863466319\n",
            "Epoch 5840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663218\n",
            "Epoch 5860 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863466324\n",
            "Epoch 5880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663262\n",
            "Epoch 5900 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663284\n",
            "Epoch 5920 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663304\n",
            "Epoch 5940 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663318\n",
            "Epoch 5960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663331\n",
            "Epoch 5980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663348\n",
            "Epoch 6000 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663356\n",
            "Epoch 6020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663373\n",
            "Epoch 6040 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663384\n",
            "Epoch 6060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663395\n",
            "Epoch 6080 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663404\n",
            "Epoch 6100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663412\n",
            "Epoch 6120 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663418\n",
            "Epoch 6140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663423\n",
            "Epoch 6160 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663431\n",
            "Epoch 6180 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663437\n",
            "Epoch 6200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663445\n",
            "Epoch 6220 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663448\n",
            "Epoch 6240 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466345\n",
            "Epoch 6260 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466346\n",
            "Epoch 6280 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663462\n",
            "Epoch 6300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663465\n",
            "Epoch 6320 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663465\n",
            "Epoch 6340 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466347\n",
            "Epoch 6360 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863466347\n",
            "Epoch 6380 | Train Loss 0.19500411767574743 | Test Loss 0.1959833863466347\n",
            "Epoch 6400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663479\n",
            "Epoch 6420 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663479\n",
            "Epoch 6440 | Train Loss 0.19500411767574746 | Test Loss 0.1959833863466348\n",
            "Epoch 6460 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663484\n",
            "Epoch 6480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663484\n",
            "Epoch 6500 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663484\n",
            "Epoch 6520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663487\n",
            "Epoch 6540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663487\n",
            "Epoch 6560 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663487\n",
            "Epoch 6580 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663487\n",
            "Epoch 6600 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663492\n",
            "Epoch 6620 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663492\n",
            "Epoch 6640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663492\n",
            "Epoch 6660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663495\n",
            "Epoch 6680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663492\n",
            "Epoch 6700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663495\n",
            "Epoch 6720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663495\n",
            "Epoch 6740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663495\n",
            "Epoch 6760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6780 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6800 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6820 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6900 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6920 | Train Loss 0.19500411767574743 | Test Loss 0.19598338634663498\n",
            "Epoch 6940 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 6980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7000 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7040 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7080 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7120 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7160 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7180 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7220 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7240 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7260 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7280 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7320 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7340 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7360 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7380 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7420 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7440 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7460 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7500 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7560 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7580 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7600 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7620 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7780 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7800 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7820 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7900 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7920 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7940 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 7980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8000 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8040 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8080 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8120 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8160 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8180 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8220 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8240 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8260 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8280 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8320 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8340 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8360 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8380 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8420 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8440 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8460 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8500 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8560 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8580 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8600 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8620 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8780 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8800 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8820 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8900 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8920 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8940 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 8980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9000 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9020 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9040 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9060 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9080 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9100 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9120 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9140 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9160 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9180 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9200 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9220 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9240 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9260 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9280 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9300 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9320 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9340 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9360 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9380 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9400 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9420 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9440 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9460 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9480 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9500 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9520 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9540 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9560 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9580 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9600 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9620 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9640 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9660 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9680 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9700 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9720 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9740 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9760 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9780 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9800 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9820 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9840 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9860 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9880 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9900 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9920 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9940 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9960 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n",
            "Epoch 9980 | Train Loss 0.19500411767574746 | Test Loss 0.19598338634663498\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 10000        # 迭代轮次\n",
        "train_loss_history = []   # 训练集每轮次损失值列表\n",
        "test_loss_history = []    # 测试集每轮次损失值列表\n",
        "w_history = []            # 每轮次参数列表\n",
        "dim = x_train.shape[0]    # 参数维度（特征向量的维度）\n",
        "num_train = x_train.shape[1]  # 训练集大小\n",
        "num_test = x_test.shape[1]    # 测试集大小\n",
        "\n",
        "\n",
        "\n",
        "model = LinearRegression(dim = dim, lr = 0.1)\n",
        "for i in range(num_epochs):\n",
        "  y_hat = model.forward(x_train)        # 对训练数据集进行预测\n",
        "  train_loss = 1/(2 * num_train) * ((y_train - y_hat) ** 2).sum()   #  计算损失值。损失函数为均方误差损失 mean squared error (MSE) loss\n",
        "\n",
        "  w_history.append(model.w)             # 保存上一轮参数w\n",
        "  model.backward(x_train,y_hat,y_train) # 对训练集数据反向传播\n",
        "  model.optimize()                      # 优化参数\n",
        "\n",
        "  y_hat = model.forward(x_test)         # 对测试数据集进行预测\n",
        "  test_loss = 1/(2 * num_test) * ((y_test - y_hat) ** 2).sum()      #  计算损失值\n",
        "\n",
        "  train_loss_history.append(train_loss) # 保存本轮训练集损失\n",
        "  test_loss_history.append(test_loss)   # 保存本轮测试集损失\n",
        "\n",
        "  if i % 20 == 0: # 每20轮打印一次\n",
        "    print(f\"Epoch {i} | Train Loss {train_loss} | Test Loss {test_loss}\") # 打印当前epoch、训练集损失、测试集损失\n",
        "\n",
        "plt.plot(range(num_epochs), train_loss_history, label = \"Training\")   # 以epoch为x轴画出train_loss曲线\n",
        "plt.plot(range(num_epochs), test_loss_history, label = \"Test\")        # 以epoch为x轴画出test_loss曲线\n",
        "plt.legend()  # 添加图例\n",
        "plt.show()    # 显示图形"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldFMBPuvr0l0"
      },
      "source": [
        "# Results\n",
        "\n",
        "Before viewing the results, we need to reverse the transformations applied on the output variable y.\n",
        "\n",
        "The `inverse_transform` method of the StandardScaler object will help us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZycI4aExMsoC",
        "outputId": "47c6b8fa-d1ee-4ff6-90d0-7e9289cfe40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Error 0.5263803029005855\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error      # 导入sklearn中的均方误差函数\n",
        "y_test = output_scalar.inverse_transform(y_test[np.newaxis,:])  # 对测试集数据进行反归一化\n",
        "y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:])   # 对最后一轮测试集的预测值进行反归一化\n",
        "error = (((y_test - y_hat) ** 2).sum() / num_test )             # 对测试集计算预测的均方误差\n",
        "print(\"Test Set Error\", error)  # 打印均方误差值"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoYobRS9uBIv"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "Instead of coding everything from scratch, i.e the model, loss functions, and gradient calculations, there are many libaries that have implemented many machine learning algorithms for us.\n",
        "\n",
        "These libraries will generally be faster and more optimized. We can use the LinearRegression and SGD regressor module from scikit learn to compare our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txWBY_0eoNN_",
        "outputId": "7886b7e0-c383-4676-e825-3d7197d06d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Error 0.5261536932841558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\25627\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = dataset_copy # 取出之前copy的归一化数据\n",
        "sgd = SGDRegressor()    # 实例化 SGDRegressor 类；采用SGD算法来优化平方损失函数\n",
        "sgd.fit(x_train.T, y_train)     # 将模型拟合到训练数据\n",
        "y_hat = sgd.predict(x_test.T)   # 在测试集上进行预测\n",
        "y_test = output_scalar.inverse_transform(y_test[np.newaxis,:])  # 对测试集数据进行反归一化\n",
        "y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:])   # 对测试集的预测值进行反归一化\n",
        "error = mean_squared_error(y_test, y_hat, squared = True)       # 对测试集计算预测的均方误差\n",
        "print(\"Test Set Error\", error)  # 打印均方误差值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CaqphG8TG7V",
        "outputId": "579bc8dd-6093-4155-8966-f8c60c34b1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Error 0.5263803029005857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\25627\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression as LR\n",
        "\n",
        "x_train, x_test, y_train, y_test = dataset_copy # 取出之前copy的归一化数据\n",
        "lr = LR()   # 实例化 LinearRegression 类；采用最小二乘法进行反向传播计算\n",
        "lr.fit(x_train.T, y_train)          # 将模型拟合到训练数据\n",
        "y_hat = lr.predict(x_test.T)        # 在测试集上进行预测\n",
        "y_test = output_scalar.inverse_transform(y_test[np.newaxis,:])  # 对测试集数据进行反归一化\n",
        "y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:])   # 对测试集的预测值进行反归一化\n",
        "error = mean_squared_error(y_test, y_hat, squared = True)       # 对测试集计算预测的均方误差\n",
        "print(\"Test Set Error\", error)  # 打印均方误差值"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyODV4REEuhLrJ1l8OWl6dFt",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Linear Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
